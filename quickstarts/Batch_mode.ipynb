{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7asVsfQvEind"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "Mkt6Qu4FEhWC"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP1Ft6CTEohS"
      },
      "source": [
        "# Gemini API - Batch Mode\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Batch_mode.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>\n",
        "\n",
        "[Batch Mode](https://ai.google.dev/gemini-api/docs/batch-mode) is designed for processing large volumes of non-latency-critical requests asynchronously. It's ideal for workloads that require high throughput, such as pre-processing datasets, running large-scale evaluations, or generating content in bulk.\n",
        "\n",
        "**Key Benefits:**\n",
        "*   **High throughput:** Process millions of requests in a single job.\n",
        "*   **Cost savings:** Batches are priced at a 50% discount compared to the standard API.\n",
        "*   **Asynchronous:** Submit your job and retrieve the results later, within a 24-hour SLO.\n",
        "\n",
        "**In this notebook, you will learn how to:**\n",
        "1.  Set up your environment for Batch Mode.\n",
        "2.  Create a batch job by uploading a JSONL file (recommended for large jobs).\n",
        "3.  Create a batch job using inline requests (convenient for smaller jobs).\n",
        "4.  Monitor the status of your job.\n",
        "5.  Retrieve and parse the results for both job types.\n",
        "6.  Manage your jobs (list and cancel)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfk6YY3G5kqp"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5027929de8f"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "Install the SDK from [PyPI](https://github.com/googleapis/python-genai)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "46zEFO2a9FFd",
        "outputId": "fbef35c6-c7da-4aa4-e61f-52a751e3e9af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/241.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q -U \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTIfnvCn9HvH"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A1pkoyZb9Jm3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hx_Gw9i0Yuv"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK you now only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HghvVpbU0Uap"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY,http_options={'api_version': 'v1alpha'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvA_mbi1JxD5"
      },
      "source": [
        "### Choose a model\n",
        "\n",
        "Most Gemini models are compatible with Batch mode, but the size of the queue is not the same for each of them. Refer to the [documentation](https://ai.google.dev/gemini-api/docs/batch-mode#technical-details) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AChpZWIXu62m"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.0-flash\", \"gemini-2.0-flash-preview-image-generation\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvhjUMUApiCy"
      },
      "source": [
        "Media gen models ([Imagen](./Get_started_imagen.ipynb), [Lyria](./Get_started_LyriaRealTime.ipynb) or [Veo](./Get_started_Veo.ipynb)) are not currently compatible with Batch API. But if you want to batch create images you can use the [Gemini 2.0 Flash Preview Image Generation](./Image_out.ipynb) model (see example below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDoq2bRzSn7k"
      },
      "source": [
        "# Creating Batch Jobs: Two Methods\n",
        "\n",
        "You can create batch jobs in two ways:\n",
        "\n",
        "1.  **File-based (`src`)**: Upload a JSONL file containing all your requests. This is the recommended method for large datasets.\n",
        "2.  **Inline (`inlined_requests`)**: Pass a list of request objects directly in your code. This is convenient for smaller, dynamically generated jobs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVcYxERES1fY"
      },
      "source": [
        "## Create a job from a file\n",
        "\n",
        "This is the most common workflow. You will prepare an input file, upload it, create the job, monitor it, and retrieve the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMT-Xx7AXebz"
      },
      "source": [
        "### Step 1: Prepare and upload the input File\n",
        "\n",
        "The input file must be a **JSON** file, where each line is a JSON object. Each object must contain a unique `key` to help you correlate inputs with outputs, and a `request` object matching the `GenerateContentRequest` schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V7xoP6tsLZ0B",
        "outputId": "fcfd5d4d-e21a-4327-aee1-e571fab9a97c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading file: batch_requests.json\n",
            "Uploaded file: files/ftbmwtukbn46\n"
          ]
        }
      ],
      "source": [
        "# Create a sample JSONL file.\n",
        "# The 'key' field is required for correlating inputs to outputs.\n",
        "\n",
        "import json\n",
        "\n",
        "requests_data = [\n",
        "    {\"key\": \"request_1\", \"request\": {\"contents\": [{\"parts\": [{\"text\": \"Explain how AI works in a few words\"}]}]}},\n",
        "    {\"key\": \"request_2\", \"request\": {\"contents\": [{\"parts\": [{\"text\": \"Explain how quantum computing works in a few words\"}]}]}}\n",
        "]\n",
        "\n",
        "json_file_path = 'batch_requests.json'\n",
        "\n",
        "with open(json_file_path, 'w') as f:\n",
        "    for req in requests_data:\n",
        "        f.write(json.dumps(req) + '\\n')\n",
        "\n",
        "# 2. Upload JSONL file to File API.\n",
        "print(f\"Uploading file: {json_file_path}\")\n",
        "uploaded_batch_requests = client.files.upload(\n",
        "    file=json_file_path,\n",
        "    config=types.UploadFileConfig(display_name='batch-input-file')\n",
        ")\n",
        "print(f\"Uploaded file: {uploaded_batch_requests.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHiIK7vBTHvp"
      },
      "source": [
        "### Step 2: Create the Batch Job\n",
        "\n",
        "Now, pass the uploaded file's name (`uploaded_batch_requests.name`) to the `client.batches.create` function to create the batch job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "js7fQU2dFRcJ",
        "outputId": "e7d3e8aa-52cc-48c5-851a-79b6f771118e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-945405536.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m batch_job_from_file = client.batches.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muploaded_batch_requests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     config={\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'display_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'my-batch-job-from-file'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/batches.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, src, config)\u001b[0m\n\u001b[1;32m   2337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m   def list(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/batches.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(self, model, src, config)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   1922\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m     response_body = (\n\u001b[1;32m   1268\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       )\n\u001b[0;32m-> 1063\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m       return HttpResponse(\n\u001b[1;32m   1065\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}"
          ]
        }
      ],
      "source": [
        "batch_job_from_file = client.batches.create(\n",
        "    model=MODEL_ID,\n",
        "    src=uploaded_batch_requests.name,\n",
        "    config={\n",
        "        'display_name': 'my-batch-job-from-file',\n",
        "    }\n",
        ")\n",
        "print(f\"Created batch job from file: {batch_job_from_file.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMn7rMKt-4vS"
      },
      "source": [
        "### Step 3: Monitor job status\n",
        "\n",
        "Jobs can take time to complete (up to 24 hours). You can poll the API to check the status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRfeqghtTdHb"
      },
      "outputs": [],
      "source": [
        "# Note: You can check the status of any job by replacing its name here.\n",
        "# For example: job_name = 'batches/your-job-name-here'\n",
        "\n",
        "import time\n",
        "\n",
        "job_name = batch_job_from_file.name\n",
        "\n",
        "print(f\"Polling status for job: {job_name}\")\n",
        "\n",
        "# Poll the job status until it's completed.\n",
        "while True:\n",
        "    batch_job = client.batches.get(name=job_name)\n",
        "    if batch_job.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED'):\n",
        "        break\n",
        "    print(f\"Job not finished. Current state: {batch_job.state.name}. Waiting 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(f\"Job finished with state: {batch_job.state.name}\")\n",
        "if batch_job.state.name == 'JOB_STATE_FAILED':\n",
        "    print(f\"Error: {batch_job.error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zbBqLUGTnJG"
      },
      "source": [
        "### Step 4: Retrieve and parse results\n",
        "\n",
        "Once a file-based job succeeds, the results are written to an output file in the [Files API](./File_API.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvpNS7r4Tt7Y"
      },
      "outputs": [],
      "source": [
        "if batch_job.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "    # The output is in another file.\n",
        "    result_file_name = batch_job.dest.file_name\n",
        "    print(f\"Results are in file: {result_file_name}\")\n",
        "\n",
        "    print(\"\\nDownloading and parsing result file content...\")\n",
        "    file_content_bytes = client.files.download(file=result_file_name)\n",
        "    file_content = file_content_bytes.decode('utf-8')\n",
        "\n",
        "    # The result file is also a JSONL file. Parse and print each line.\n",
        "    for line in file_content.splitlines():\n",
        "      if line:\n",
        "        parsed_response = json.loads(line)\n",
        "        # Pretty-print the JSON for readability\n",
        "        print(json.dumps(parsed_response, indent=2))\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(f\"Job did not succeed. Final state: {batch_job.state.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np2wyrdSTzpF"
      },
      "source": [
        "## Alternative: Create a job with inline requests\n",
        "\n",
        "For smaller tasks, you can pass requests directly without creating a file. The results will be returned directly in the job object itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_eo0MpbXZN9"
      },
      "source": [
        "### Step 1: Create and monitor the inline job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUQb83QUUPXi"
      },
      "outputs": [],
      "source": [
        "# 1. Define your list of requests.\n",
        "# Note: Unlike the file-based method, a 'key' is not required for inline requests,\n",
        "# as the order of responses will match the order of requests.\n",
        "inline_requests_list = [\n",
        "    {'contents': [{'parts': [{'text': 'Write a short poem about a cloud.'}]}]},\n",
        "    {'contents': [{'parts': [{'text': 'Write a short poem about a cat.'}]}]}\n",
        "]\n",
        "\n",
        "# 2. Create the batch job with the inline requests.\n",
        "print(\"Creating inline batch job...\")\n",
        "batch_job_inline = client.batches.create(\n",
        "    model=MODEL_ID,\n",
        "    src=inline_requests_list,\n",
        "    config={'display_name': 'my-batch-job-inline-example'}\n",
        ")\n",
        "print(f\"Created inline batch job: {batch_job_inline.name}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# 3. Monitor the job until completion.\n",
        "job_name = batch_job_inline.name\n",
        "print(f\"Polling status for job: {job_name}\")\n",
        "\n",
        "while True:\n",
        "    batch_job_inline = client.batches.get(name=job_name)\n",
        "    if batch_job_inline.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED'):\n",
        "        break\n",
        "    print(f\"Job not finished. Current state: {batch_job.state.name}. Waiting 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(f\"Job finished with state: {batch_job.state.name}\")\n",
        "if batch_job.state.name == 'JOB_STATE_FAILED':\n",
        "    print(f\"Error: {batch_job.error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp2HRIakWuP6"
      },
      "source": [
        "### Step 2: Retrieve and print inline results\n",
        "Once the job has succeeded, the results are available in the inlined_responses field of the job object. You can iterate through this list to get each response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4N_CD6pW0vC"
      },
      "outputs": [],
      "source": [
        "if batch_job_inline.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "    print(\"\\nResults are inline:\")\n",
        "    # The results are in the `inlined_responses` field.\n",
        "    for i, inline_response in enumerate(batch_job_inline.dest.inlined_responses):\n",
        "        print(f\"\\n--- Response {i+1} ---\")\n",
        "\n",
        "        # Check for a successful response\n",
        "        if inline_response.response:\n",
        "            # The .text property is a shortcut to the generated text.\n",
        "            try:\n",
        "                print(inline_response.response.text)\n",
        "            except AttributeError:\n",
        "                # Fallback to printing the full response if .text isn't available\n",
        "                print(inline_response.response)\n",
        "\n",
        "        # Check for an error in this specific request\n",
        "        elif inline_response.error:\n",
        "            print(f\"Error: {inline_response.error}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Job did not succeed. Final state: {batch_job_inline.state.name}\")\n",
        "    if batch_job_inline.error:\n",
        "        print(f\"Error: {batch_job_inline.error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyhvRvroUTNY"
      },
      "source": [
        "## Managing jobs\n",
        "\n",
        "Here are some common operations for managing your batch jobs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVItnYVmUZIA"
      },
      "source": [
        "### List your batch jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2audPYLoHxC"
      },
      "source": [
        "#### List Recent Jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o40vd32JUbTo"
      },
      "outputs": [],
      "source": [
        "print(\"Listing recent batch jobs:\\n\")\n",
        "\n",
        "# Note: The list API currently doesn't return inlined_responses.\n",
        "# As a workaround,you can make a `get` call for inline jobs to see their results.\n",
        "batches = client.batches.list(config={'page_size': 10})\n",
        "\n",
        "for b in batches.page:\n",
        "    print(f\"Job Name: {b.name}\")\n",
        "    print(f\"  - Display Name: {b.display_name}\")\n",
        "    print(f\"  - State: {b.state.name}\")\n",
        "    print(f\"  - Create Time: {b.create_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Check if it was an inline job (no destination file)\n",
        "    if b.dest is not None:\n",
        "      if not b.dest.file_name:\n",
        "        full_job = client.batches.get(name=b.name)\n",
        "        if full_job.inlined_responses:\n",
        "            print(\"  - Type: Inline ({} responses)\".format(len(full_job.inlined_responses)))\n",
        "      else:\n",
        "          print(f\"  - Type: File-based (Output: {b.dest.file_name})\")\n",
        "\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slbsTW2dUnou"
      },
      "source": [
        "#### Cancel a Batch Job (Optional)\n",
        "\n",
        "If you need to stop a job that is still pending or running, you can cancel it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0YqOsIqoRBq"
      },
      "source": [
        "#### Cancel a Job\n",
        "The next cell is commented out to prevent accidental cancellation.\n",
        "\n",
        "To use it:\n",
        "1. Get the name of the job you want to cancel (e.g., from the list above).\n",
        "2. Uncomment the code and replace the placeholder name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpMVDjwpUqZ0"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   job_to_cancel_name = \"batches/your-job-name-here\" # <-- REPLACE THIS\n",
        "#   print(f\"Attempting to cancel job: {job_to_cancel_name}\")\n",
        "#   client.batches.cancel(name=job_to_cancel_name)\n",
        "#   print(\"Job cancellation request sent.\")\n",
        "# except Exception as e:\n",
        "#   print(f\"Error cancelling job: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71y99gD9YeE3"
      },
      "source": [
        "## Other examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-QocoN6Yg_I"
      },
      "source": [
        "### Using multimodal input\n",
        "Here's an example using multimodal input. Once again it will use the [Files API](./File_API.ipynb) to store the image you want to send along with your prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKWc0M6bYkyb"
      },
      "outputs": [],
      "source": [
        "# Download sample image\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "image_path = \"jetpack.jpg\"\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg -O {image_path}  -q\n",
        "\n",
        "print(f\"Uploading image file: {image_path}\")\n",
        "image_file = client.files.upload(\n",
        "    file=image_path,\n",
        ")\n",
        "print(f\"Uploaded image file: {image_file.name} with MIME type: {image_file.mime_type}\")\n",
        "Image(filename=image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67gGuZXoYvU-"
      },
      "outputs": [],
      "source": [
        "requests_data = [\n",
        "    # First request: simple text prompt\n",
        "    {\"key\": \"request_1\", \"request\": {\"contents\": [{\"parts\": [{\"text\": \"Explain how AI works in a few words\"}]}]}},\n",
        "    # Second request: multi-modal prompt with text and an image reference\n",
        "    {\n",
        "        \"key\": \"request_2_image\",\n",
        "        \"request\": {\n",
        "            \"contents\": [{\n",
        "                \"parts\": [\n",
        "                    {\"text\": \"What is in this image? Describe it in detail.\"},\n",
        "                    {\"file_data\": {\"file_uri\": image_file.uri, \"mime_type\": image_file.mime_type}}\n",
        "                ]\n",
        "            }]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "json_file_path = 'batch_requests_with_image.json'\n",
        "\n",
        "print(f\"\\nCreating JSONL file: {json_file_path}\")\n",
        "with open(json_file_path, 'w') as f:\n",
        "    for req in requests_data:\n",
        "        f.write(json.dumps(req) + '\\n')\n",
        "\n",
        "print(f\"Uploading JSONL file: {json_file_path}\")\n",
        "batch_input_file = client.files.upload(\n",
        "    file=json_file_path\n",
        "    )\n",
        "print(f\"Uploaded JSONL file: {batch_input_file.name}\")\n",
        "\n",
        "print(\"\\nCreating batch job...\")\n",
        "batch_job_from_file = client.batches.create(\n",
        "    model=MODEL_ID,\n",
        "    src=batch_input_file.name,\n",
        "    config={\n",
        "        'display_name': 'my-batch-job-with-image',\n",
        "    }\n",
        ")\n",
        "print(f\"Created batch job from file: {batch_job_from_file.name}\")\n",
        "print(\"You can now monitor the job status using its name.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9ArGYNY6fbD"
      },
      "outputs": [],
      "source": [
        "batch_job_from_file = client.batches.get(name=batch_job_from_file.name)\n",
        "if batch_job_from_file.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "    # The output is in another file.\n",
        "    result_file_name = batch_job_from_file.dest.file_name\n",
        "    print(f\"Results are in file: {result_file_name}\")\n",
        "\n",
        "    print(\"\\nDownloading and parsing result file content...\")\n",
        "    file_content_bytes = client.files.download(file=result_file_name)\n",
        "    file_content = file_content_bytes.decode('utf-8')\n",
        "\n",
        "    # The result file is also a JSONL file. Parse and print each line.\n",
        "    for line in file_content.splitlines():\n",
        "      if line:\n",
        "        parsed_response = json.loads(line)\n",
        "        # Pretty-print the JSON for readability\n",
        "        print(json.dumps(parsed_response['response']['candidates'][0]['content'], indent=2))\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(f\"Job did not succeed. Final state: {batch_job_from_file.state.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYmN6UFIpIbJ"
      },
      "source": [
        "### Multimodal output\n",
        "This time you're going to batch generate images using the [Gemini 2.5 Image Generation](https://ai.google.dev/gemini-api/docs/image-generation#gemini) model. More details on the [dedicated notebook](./Image_out.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy22E3YTp6I2"
      },
      "outputs": [],
      "source": [
        "requests_data = [\n",
        "    {\"key\": \"image_request_1\", \"request\": {\"contents\": [{\"parts\": [{\"text\": \"A big letter A surrounded by animals starting with the A letter\"}]}],'generation_config': {'response_modalities': ['TEXT', 'IMAGE']}}}, # Be careful, for inline request it would be 'config' instead of 'generation_config'\n",
        "    {\"key\": \"image_request_2\", \"request\": {\"contents\": [{\"parts\": [{\"text\": \"A big letter B surrounded by animals starting with the B letter\"}]}],'generation_config': {'response_modalities': ['TEXT', 'IMAGE']}}},\n",
        "]\n",
        "\n",
        "json_file_path = 'batch_image_gen_requests.json'\n",
        "\n",
        "print(f\"\\nCreating JSONL file: {json_file_path}\")\n",
        "with open(json_file_path, 'w') as f:\n",
        "    for req in requests_data:\n",
        "        f.write(json.dumps(req) + '\\n')\n",
        "\n",
        "print(f\"Uploading JSONL file: {json_file_path}\")\n",
        "batch_input_file = client.files.upload(\n",
        "    file=json_file_path\n",
        "    )\n",
        "print(f\"Uploaded JSONL file: {batch_input_file.name}\")\n",
        "\n",
        "print(\"\\nCreating batch job...\")\n",
        "batch_multimodal_job = client.batches.create(\n",
        "    model=\"gemini-2.5-flash-image-preview\",\n",
        "    src=batch_input_file.name,\n",
        "    config={\n",
        "        'display_name': 'my-batch-image-gen-job',\n",
        "    }\n",
        ")\n",
        "print(f\"Created batch job from file: {batch_multimodal_job.name}\")\n",
        "print(\"You can now monitor the job status using its name.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU_8OdgNwQfC"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown, Image\n",
        "import base64\n",
        "\n",
        "batch_multimodal_job = client.batches.get(name=batch_multimodal_job.name)\n",
        "\n",
        "if batch_multimodal_job.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "    # The output is in another file.\n",
        "    result_file_name = batch_multimodal_job.dest.file_name\n",
        "    print(f\"Results are in file: {result_file_name}\")\n",
        "\n",
        "    print(\"\\nDownloading and parsing result file content...\")\n",
        "    file_content_bytes = client.files.download(file=result_file_name)\n",
        "    file_content = file_content_bytes.decode('utf-8')\n",
        "\n",
        "    # The result file is also a JSONL file. Parse and print each line.\n",
        "    for line in file_content.splitlines():\n",
        "      if line:\n",
        "        parsed_response = json.loads(line)\n",
        "        for part in parsed_response['response']['candidates'][0]['content']['parts']:\n",
        "          if part.get('text'):\n",
        "            display(Markdown(part['text']))\n",
        "          elif part.get('inlineData'):\n",
        "            mime = part['inlineData']['mimeType']\n",
        "            data = base64.b64decode(part['inlineData']['data'])\n",
        "            display(Image(data=data, format=mime.split('/')[-1]))\n",
        "else:\n",
        "    print(f\"Job did not succeed. Final state: {batch_multimodal_job.state.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx6_4C3huMx-"
      },
      "source": [
        "The output could not be saved in the notebook but here's what it could look like:\n",
        "![Letter A](https://storage.googleapis.com/generativeai-downloads/cookbook/Batch_mode/letter-A.png)\n",
        "\n",
        "![Letter B](https://storage.googleapis.com/generativeai-downloads/cookbook/Batch_mode/letter-B.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4677dd58e9b5"
      },
      "source": [
        "## Next Steps\n",
        "### Useful API references:\n",
        "\n",
        "For more details on Batch mode, please check the related [documentation](https://ai.google.dev/gemini-api/docs/batch-mode).\n",
        "\n",
        "### Continue your discovery of the Gemini API\n",
        "\n",
        "Check other nice Gemini capabilities in the [Cookbook](https://github.com/google-gemini/cookbook/). In particular, the [quickstarts](https://github.com/google-gemini/cookbook/quickstarts) folder is full of guides on how to use the Gemini (and gen-media) models and features, while the [example](https://github.com/google-gemini/cookbook/examples) folder showcase coiol use-cases to spark your creativity.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Batch_mode.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}